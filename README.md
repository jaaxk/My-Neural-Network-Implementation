This project was a way for me to understand the very specifics of how existing Python libraries like tensorflow and scikit-learn work. The following are the functions that I've implemented:

Activation Functions:

Relu,
Relu Derivative,
Softmax

Loss Functions:

Cross entropy

Optimization Functions:

Random Search,
Stochastic Gradient Descent (unfinished, trying to work out backpropagation)

Preprocessing:

To Categorical,
Normalize

The tests.py and tests.ipynb are an example of how to use this module on an example dataset, WineQT.csv.
There is no requirements.txt for this module, as the only required library is numpy.

This project allowed me to learn a lot about neural networks, loss/optimization/activation functions, Object Oriented Programming with Python, preprocessing hunctions and more.
I hope to add more features to this project as I learn more about ML and NNs.
I hope someone else is able to learn from this.

